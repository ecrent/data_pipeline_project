# ğŸš€ Batch Processing Data Pipeline

A scalable, containerized data pipeline for processing e-commerce user events and generating customer profiles for machine learning applications.

## ğŸ—ï¸ Architecture Overview

This project implements a modern data engineering pipeline using a microservices architecture with the following components:

```mermaid
graph LR
    A[CSV Data] --> B[Ingestion Service]
    B --> C[Apache Kafka]
    C --> D[Spark Processing]
    D --> E[MinIO Data Lake]
    D --> F[Elasticsearch]
    F --> G[Kibana Dashboard]
    
    H[Zookeeper] --> C
    I[Spark Master] --> D
    J[Spark Worker] --> D
```

### Core Components

- **ğŸ”„ Apache Kafka**: Message streaming platform for reliable data ingestion
- **âš¡ Apache Spark**: Distributed processing engine for data transformation and aggregation
- **ğŸ—„ï¸ MinIO**: S3-compatible object storage for data lake functionality  
- **ğŸ” Elasticsearch**: Analytics database for storing processed customer profiles
- **ğŸ“Š Kibana**: Visualization platform for exploring customer data
- **ğŸ Python Services**: Custom ingestion and processing applications

## ğŸŒ Multi-Environment Support

This pipeline works seamlessly in both **GitHub Codespaces** and **local WSL** environments:

### GitHub Codespaces
- Automatic environment detection
- Dynamic URL generation (e.g., `https://super-duper-waffle-pvqvvr74jxg3rx-5601.app.github.dev/`)
- Port visibility configuration
- HTTPS endpoints with proper forwarding

### Local Development (WSL)
- Standard localhost URLs (e.g., `http://localhost:5601/`)
- Direct container networking
- Optimized for local development

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Detect environment and generate service URLs
./setup-environment.sh

# Configure port visibility (Codespaces only)
./configure-ports.sh
```

### 2. Dataset Setup
```bash
# Download the dataset manually from Kaggle:
# 1. Visit: https://www.kaggle.com/mkechinov/ecommerce-events-history-in-electronics-store
# 2. Download the main CSV file
# 3. Save it as ./data/electronics.csv

# Verify dataset is ready
make check-data
```

### 3. Start Core Services
```bash
# GitHub Codespaces
docker-compose -f docker-compose.yml -f docker-compose.codespaces.yml up -d

# Local/WSL
docker-compose up -d

# Or use Make command
make start
```

### 4. Run the Pipeline
```bash
# Start data ingestion (CSV -> Kafka)
make ingest

# Start data processing (Kafka -> Spark -> MinIO + Elasticsearch)
make process
```

## ğŸ“‚ Project Structure

```
data_pipeline_project/
â”œâ”€â”€ ğŸ³ docker-compose.yml           # Main orchestration file
â”œâ”€â”€ ğŸ³ docker-compose.codespaces.yml # Codespaces-specific overrides
â”œâ”€â”€ âš™ï¸ .env                         # Environment variables
â”œâ”€â”€ ğŸ“‹ setup-environment.sh         # Environment detection & setup
â”œâ”€â”€ ğŸ”§ configure-ports.sh           # Codespaces port configuration
â”‚
â”œâ”€â”€ ğŸ“Š data/                        # Source data directory
â”‚   â””â”€â”€ electronics.csv             # Sample e-commerce data
â”‚
â”œâ”€â”€ ğŸ”„ ingestion/                   # Data ingestion service
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ kafka_producer.py
â”‚
â”œâ”€â”€ âš¡ processing/                   # Spark processing application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ data_processor.py
â”‚       â””â”€â”€ customer_profiles.py
â”‚
â”œâ”€â”€ ğŸ¯ spark/                       # Spark configuration
â”‚   â”œâ”€â”€ apps/                       # Spark applications
â”‚   â””â”€â”€ jars/                       # Additional JAR dependencies
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                   # Jupyter notebooks for analysis
â”œâ”€â”€ âš™ï¸ config/                      # Additional configurations
â””â”€â”€ ğŸ“š docs/                        # Project documentation
```

## ğŸ”§ Service Configuration

### Service Ports
| Service | Local Port | Description |
|---------|------------|-------------|
| Kibana | 5601 | Analytics dashboard |
| Spark Master UI | 8080 | Cluster monitoring |
| Spark Worker UI | 8081 | Worker node status |
| MinIO Console | 9001 | Storage management |
| MinIO API | 9000 | Object storage API |
| Elasticsearch | 9200 | Search & analytics |
| Kafka | 9092 | Message streaming |
| Zookeeper | 2181 | Kafka coordination |

### Environment Variables
Key configuration options in `.env`:
- `ENVIRONMENT`: Automatically set to `codespaces` or `local`
- `KAFKA_TOPIC`: Default topic for raw events
- `MINIO_BUCKET`: Data lake bucket name
- `ELASTICSEARCH_INDEX`: Customer profiles index

## ğŸ“ˆ Data Flow

1. **Ingestion**: Python service reads CSV and publishes JSON messages to Kafka
2. **Streaming**: Kafka buffers and distributes events to processing services
3. **Processing**: Spark application:
   - Consumes events from Kafka
   - Cleans and validates data
   - Archives raw data to MinIO (Parquet format)
   - Aggregates customer metrics
   - Stores profiles in Elasticsearch
4. **Visualization**: Kibana provides dashboards for exploring customer profiles

## ğŸ› ï¸ Development Workflow

### Phase 1: Infrastructure Setup âœ…
- [x] Multi-environment docker-compose configuration
- [x] Environment detection and URL generation
- [x] Port configuration for Codespaces
- [x] Service orchestration with health checks

### Phase 2: Data Acquisition & Ingestion âš¡
- [x] Manual dataset acquisition setup
- [x] Dataset validation and preprocessing
- [ ] Python Kafka producer service
- [ ] CSV parsing and JSON transformation
- [ ] Error handling and data validation
- [ ] Configurable batch processing

### Phase 3: Data Processing
- [ ] Spark application development
- [ ] Data cleaning and validation logic
- [ ] Customer profile aggregation
- [ ] MinIO integration for data lake
- [ ] Elasticsearch indexing

### Phase 4: Visualization & Monitoring
- [ ] Kibana dashboard configuration
- [ ] Custom visualizations for customer profiles
- [ ] Service monitoring and alerting
- [ ] Performance optimization

## ğŸ”§ Common Commands

```bash
# Start all services
docker-compose up -d

# View service logs
docker-compose logs -f [service-name]

# Run ingestion service
docker-compose --profile ingestion up ingestion-service

# Run Spark processing
docker-compose --profile processing up spark-processor

# Stop all services
docker-compose down

# Remove all data (clean restart)
docker-compose down -v

# Check service health
docker-compose ps
```

## ğŸ› Troubleshooting

### Codespaces Issues
- **Services not accessible**: Run `./configure-ports.sh` to set port visibility
- **URLs not working**: Check the 'Ports' tab in VS Code and ensure ports are public
- **Slow startup**: Services may take longer in Codespaces; check logs for readiness

### Local Issues
- **Port conflicts**: Ensure no other services are using the configured ports
- **Memory issues**: Elasticsearch requires at least 2GB RAM; adjust if needed
- **Network issues**: Check Docker network configuration if services can't communicate

### General Issues
- **Service dependencies**: Wait for health checks before starting dependent services
- **Volume permissions**: Ensure Docker has permissions to create volumes
- **Resource limits**: Monitor Docker resource usage for optimal performance

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test in both Codespaces and local environments
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**ğŸ’¡ Tip**: Always run `./setup-environment.sh` after cloning to detect your environment and get the correct service URLs!